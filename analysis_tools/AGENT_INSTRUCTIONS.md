# Agent Instructions for `analyze_three_agent_results.py`

## Purpose
This script analyzes the execution logs of the Three-Agent Expert system (Differential Agent, Decision Agent, Memory Agent). It generates a detailed Markdown report containing metrics on agent performance, reasoning trajectories, and consistency.

## Usage
To run the analysis, execute the following command:

```bash
python3 analysis_tools/analyze_three_agent_results.py <INPUT_JSONL_FILE> --output <OUTPUT_MARKDOWN_FILE>
```

### Arguments
- `<INPUT_JSONL_FILE>`: Path to the `results.jsonl` file generated by the agent run (e.g., `outputs/ThreeAgentExpert_.../results.jsonl`).
- `--output <OUTPUT_MARKDOWN_FILE>`: (Optional but Recommended) Path to save the generated Markdown report.

## Output Report Sections
The generated report contains several key sections that you should analyze:

### 1. Memory Agent Performance
- **Metrics**: Interventions, Assess Progress Calls, Strategy Suggestions.
- **Goal**: Evaluate if the Memory Agent is correctly identifying when the patient provides no new info and suggesting valid strategies.

### 2. Question Asking Quality
- **Metrics**: Total Questions, Redundant Questions, Failed Questions (No Info).
- **Goal**: Check if the agent is asking unique, effective questions. High redundancy or failure rates indicate poor strategy.

### 3. Decision Ability & Context Usage
- **Metrics**: Context %, Trajectory (e.g., "Stable Correct", "Fluctuating"), Final Result.
- **Goal**: Determine if the agent is gathering enough context and converging on the correct answer.

### 4. Differential Agent Analysis
- **Metrics**:
  - **Kept Correct?**: "Yes" if the correct answer was never ruled out. "No" if it was wrongly eliminated.
  - **Option Count Trajectory**: How the number of options decreased (e.g., `4 -> 2 -> 1`).
  - **Ruled Out Reason**: If "Kept Correct?" is "No", this explains WHY the agent ruled it out.
- **Interpretation**: 
  - If "Kept Correct?" is "No", the Differential Agent failed its primary task. Analyze the "Ruled Out Reason" to understand the logic error.
  - A trajectory like `4 -> 1` in one step suggests aggressive pruning. `4 -> 3 -> 2 -> 1` suggests cautious pruning.

### 5. High Information Questions
- **Content**: Lists questions where the patient gave a long response (>100 chars).
- **Goal**: Identify which questions were most effective at eliciting information.

### 6. Confidence Analysis
- **Metrics**:
  - **Final Confidence**: The confidence score (0-100) of the final decision.
  - **Confidence Trajectory**: A list of confidence scores from all `ask_question` and `make_choice` steps (e.g., `[30.0, 60.0, 85.0]`).
  - **Intermediate Choices Correct?**: "Yes" if the agent's intermediate guesses included the correct answer.
- **Interpretation**:
  - **Ideal Trajectory**: Monotonic increase (e.g., `[30, 50, 80, 95]`).
  - **Bad Trajectory**: Fluctuating (e.g., `[80, 40, 90]`) or flat low confidence.
  - **Overconfidence**: High confidence early but wrong answer.
  - **Underconfidence**: Correct answer but low confidence throughout.

## Example Analysis Workflow
1. **Run the script** on the latest results file.
2. **Read Section 4 (Differential)**: Did we rule out the correct answer? If so, why?
3. **Read Section 6 (Confidence)**: Did confidence grow steadily? Was the agent confident in its final wrong decision?
4. **Read Section 2 (Questions)**: Were there too many redundant questions?
5. **Synthesize**: Combine these insights to recommend improvements to the agent prompts or logic.
